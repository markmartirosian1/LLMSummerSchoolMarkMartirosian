{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zk0ARdfBYUy3"
   },
   "source": [
    "\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will:\n",
    "1. Implement two complementary inference strategies:\n",
    "   - Multiple generator calls (parallel, refinement)\n",
    "   - Single long output generation (extended CoT)\n",
    "2. Integrate feedback information (reward models, LLM feedback)\n",
    "3. Compare and analyze different approaches\n",
    "4. Understand compute-accuracy trade-offs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97u2FJrDkN8_"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_KrcWiFkd0D"
   },
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFvd5N6zYJ9o",
    "outputId": "1b50f120-9db9-4d04-9c83-ff6c2d54fc7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.97.1)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install openai transformers torch datasets matplotlib seaborn numpy pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bohoD75Ye0A",
    "outputId": "f697cd6e-19ae-403f-8636-bc61af83e8e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from collections import defaultdict, Counter\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from dataclasses import dataclass\n",
    "from itertools import product\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# For CPU-only environments, we'll use smaller models\n",
    "assert device != \"cpu\", \"We need GPUs for this practice!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yrMdc7JfwVf"
   },
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKU-AQRJsxDy"
   },
   "source": [
    "#### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "g-nGQlSGfwHS",
    "outputId": "0245020e-d13a-434e-dfba-758e93a92a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen3-0.6B...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2-3211751601.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading {SELECTED_MODEL}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSELECTED_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSELECTED_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Model options - pick one!\n",
    "MODEL_OPTIONS = {\n",
    "    \"small\": \"Qwen/Qwen3-0.6B\",   # 600M params\n",
    "    \"medium\": \"Qwen/Qwen3-1.7B\",  # 1.7B params\n",
    "    \"large\": \"Qwen/Qwen3-4B\",     # 4B params\n",
    "}\n",
    "\n",
    "\n",
    "MODEL_SIZE = \"small\"\n",
    "SELECTED_MODEL = MODEL_OPTIONS[MODEL_SIZE]\n",
    "\n",
    "\n",
    "print(f\"Loading {SELECTED_MODEL}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(SELECTED_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(SELECTED_MODEL)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "generator = pipeline(\n",
    "  \"text-generation\",\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "  device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnP8W16wnn17"
   },
   "source": [
    "#### Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Xpknr75Bnm1-"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, prompt: str | list[dict[str, str]], max_tokens=100, temperature=0.7):\n",
    "    \"\"\"Helper function to generate text\"\"\"\n",
    "\n",
    "    if isinstance(prompt, str):\n",
    "      prompt = [\n",
    "          {\"role\": \"system\", \"content\": \"Please reason step by step, and put your final answer within \\\\boxed{}.\"},\n",
    "          {\"role\": \"user\", \"content\": prompt},\n",
    "      ]\n",
    "\n",
    "    result = model(\n",
    "        prompt,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        return_full_text=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "\n",
    "def extract_answer(text: str) -> int:\n",
    "    \"\"\"Extract numerical answer from text\"\"\"\n",
    "    numbers = re.findall(r'\\b\\d+\\b', text)\n",
    "    return int(numbers[-1]) if numbers else None\n",
    "\n",
    "\n",
    "def plot(data: Dict[str, List[Dict[str, Any]]], title: str):\n",
    "    \"\"\"\n",
    "    Plot line graphs for multiple datasets side by side.\n",
    "\n",
    "    Args:\n",
    "        data: Dictionary with dataset names as keys and list of dicts as values.\n",
    "              Each dict should have 'n', 'accuracy', and 'method' keys.\n",
    "        title: Overall title for the plot\n",
    "    \"\"\"\n",
    "    # Set up the plotting style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(data), figsize=(6 * len(data), 5), sharex=True)\n",
    "\n",
    "    if len(data) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (name, dataset) in zip(axes, data.items()):\n",
    "        df = pd.DataFrame(dataset)\n",
    "        sns.lineplot(data=df, x='n', y='accuracy', hue='method', marker='o', ax=ax)\n",
    "        ax.set_title(name)\n",
    "        ax.legend(title='Method')\n",
    "\n",
    "    plt.suptitle(title, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFJXsYXrYltK"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alRe0_uck7Xb"
   },
   "source": [
    "We'll use a curated set of multiplication and math problems for our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "My_XWTk3jcic"
   },
   "source": [
    "#### Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtLyYUipYiGo"
   },
   "outputs": [],
   "source": [
    "def generate_multiplication_data(digits1: int, digits2: int, size: Optional[int] = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate multiplication data with inputs of specified number of digits.\n",
    "\n",
    "    Args:\n",
    "        digits1: Number of digits for the first input\n",
    "        digits2: Number of digits for the second input\n",
    "        size: If specified, randomly sample this many combinations from all possible pairs\n",
    "              If None, generate all possible combinations\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries with format: {\"problem\": \"What is A x B?\", \"inputs\": [A, B], \"answer\": C}\n",
    "    \"\"\"\n",
    "    # Generate ranges for each operand based on digit count\n",
    "    min1 = 10**(digits1 - 1) if digits1 > 1 else 1\n",
    "    max1 = 10**digits1 - 1\n",
    "\n",
    "    min2 = 10**(digits2 - 1) if digits2 > 1 else 1\n",
    "    max2 = 10**digits2 - 1\n",
    "\n",
    "    # Generate all possible operands\n",
    "    operands1 = list(range(min1, max1 + 1))\n",
    "    operands2 = list(range(min2, max2 + 1))\n",
    "\n",
    "    if size is None:\n",
    "        # Generate all combinations\n",
    "        data = []\n",
    "        for op1 in operands1:\n",
    "            for op2 in operands2:\n",
    "                data.append({\n",
    "                    \"problem\": f\"What is {op1} x {op2}?\",\n",
    "                    \"inputs\": [op1, op2],\n",
    "                    \"answer\": op1 * op2\n",
    "                })\n",
    "        return data\n",
    "    else:\n",
    "        # Randomly sample combinations\n",
    "        total_combinations = len(operands1) * len(operands2)\n",
    "\n",
    "        if size > total_combinations:\n",
    "            print(f\"Warning: Requested size ({size}) exceeds total combinations ({total_combinations})\")\n",
    "            size = total_combinations\n",
    "\n",
    "        # Generate random samples\n",
    "        data = []\n",
    "        sampled_pairs = set()\n",
    "\n",
    "        while len(data) < size:\n",
    "            op1 = random.choice(operands1)\n",
    "            op2 = random.choice(operands2)\n",
    "\n",
    "            # Avoid duplicates\n",
    "            if (op1, op2) not in sampled_pairs:\n",
    "                sampled_pairs.add((op1, op2))\n",
    "                data.append({\n",
    "                    \"problem\": f\"What is {op1} x {op2}?\",\n",
    "                    \"inputs\": [op1, op2],\n",
    "                    \"answer\": op1 * op2\n",
    "                })\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0H_LKukjiq2"
   },
   "source": [
    "#### Generate multiplication data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-sX9OjKjaPU",
    "outputId": "4a274905-2324-4ddf-f690-7a51decb3e83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 100 multiplication problems.\n"
     ]
    }
   ],
   "source": [
    "DATA_SIZE = 100\n",
    "mul_data_4x5 = generate_multiplication_data(digits1=4, digits2=5, size=DATA_SIZE // 4)\n",
    "mul_data_5x5 = generate_multiplication_data(digits1=5, digits2=5, size=DATA_SIZE // 4)\n",
    "mul_data_5x6 = generate_multiplication_data(digits1=5, digits2=6, size=DATA_SIZE // 4)\n",
    "mul_data_7x5 = generate_multiplication_data(digits1=7, digits2=5, size=DATA_SIZE // 4)\n",
    "\n",
    "mul_data = mul_data_4x5 + mul_data_5x5 + mul_data_5x6 + mul_data_7x5\n",
    "\n",
    "print(f\"Generated {len(mul_data)} multiplication problems.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RF-Khh6sluuk"
   },
   "source": [
    "#### Load MATH-500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLSj_FkWl5Kr",
    "outputId": "87aac93b-a04b-404e-960e-c114e3582301"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 MATH-500 problems.\n"
     ]
    }
   ],
   "source": [
    "math_dataset = load_dataset(\"HuggingFaceH4/MATH-500\", split=\"test\").take(50)\n",
    "print(f\"Loaded {len(math_dataset)} MATH-500 problems.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0d35sjYqpz6c"
   },
   "source": [
    "#### Test Data Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5UN7JLipe9n"
   },
   "outputs": [],
   "source": [
    "TEST_DATA = {\n",
    "    \"multiplication\": mul_data,\n",
    "    \"math_50\": math_dataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgFkJOUplLm8"
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "534Oxtq3lKQ5"
   },
   "outputs": [],
   "source": [
    "# Sample an example response\n",
    "test_response = generate_text(TEST_DATA[\"multiplication\"][-1])\n",
    "print(f\"Test Response:\\n\\n{test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1_qJg7MtG7Z"
   },
   "source": [
    "## Before You Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cs0xMJH9q3Bi"
   },
   "source": [
    "**Model**\n",
    "\n",
    "MODEL is already loaded, use `generate_text` to sample from the model. Here is an example:\n",
    "```python\n",
    "generate_text(MODEL, \"Hi, How are you?\", max_tokens=100, temperature=1.0)\n",
    "```\n",
    "The output is a string.\n",
    "\n",
    "The model is instructed to generate its step-by-step reasoning and put its answer within `\\boxed{}`, e.g. `\\boxed{345}`.\n",
    "\n",
    "There is also a method named `extract_answer` to extract the numerical answer from model outputs.\n",
    "\n",
    "**Data**\n",
    "TEST_DATA is a dict whose key is the dataset name and its value contains the examples. The datasets are:\n",
    "\n",
    "  - `multiplication`: 100 examples stored as a list of dict with the following format:\n",
    "```json\n",
    "{\n",
    "    \"problem\": \"What is {A} x {B}?\"\n",
    "    \"inputs\": [A, B],\n",
    "    \"answer\": C,\n",
    "}\n",
    "```\n",
    "  - `MATH-500`: 50 math problems (see [here](https://huggingface.co/datasets/HuggingFaceH4/aime_2024)) that are stored as a hf Dataset object with the following format:\n",
    "```json\n",
    "{\n",
    "    \"id\": ID,\n",
    "    \"problem\": \"Define...\",\n",
    "    \"solution\": \"...\",\n",
    "    \"answer\": \"X\",\n",
    "}\n",
    "```\n",
    "\n",
    "Note that we only need \"problem\" and \"answer\" from both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80g7hqza2jG5"
   },
   "outputs": [],
   "source": [
    "N_SAMPLES1 = 4 # @param {type:\"integer\"}\n",
    "MAX_TOKENS1 = 512 # @param {type:\"integer\"}\n",
    "\n",
    "TEMPERATURE1 = 0.6 # @param {type:\"number\"}\n",
    "assert 1.0 >= TEMPERATURE1 >= 0.0, \"temperature must be between 0.0 and 1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txYLZ8IQf8pe"
   },
   "source": [
    "# Part 1: Majority Voting\n",
    "\n",
    "## Your task:\n",
    "  1. **Implement parallel generation:** Given a problem, sample multiple responses from the model\n",
    "\n",
    "  2. **Run paralllel generation on the test data:** Use the method implemented in the previous step to collect samples for the entire test data.\n",
    "\n",
    "  3. **Implement majority voting for each dataset and report accuracy:** Set `n_samples=4`, `temperature=0.6` and `max_tokens=512`.\n",
    "\n",
    "  4. **Vary `n_samples` from 1 to 32 and report the trend**\n",
    "\n",
    "  5. **Bonus: How does the results change if we increase `temperature` to 1.0?**\n",
    "\n",
    "  6. **Bonus: How about increasing `max_tokens` to 2048?**\n",
    "\n",
    "## Considerations\n",
    "\n",
    "  - The model does NOT always generate an answer for various reasons including (i) not following the instructions or (ii) reaching its context limit. Your code should work for these cases as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8i9BT2Yt-hV"
   },
   "source": [
    "## Your code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jK3gOgA1jSz"
   },
   "source": [
    "### Parallel generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lxAP3VdfJ9A"
   },
   "outputs": [],
   "source": [
    "def sample_solutions(problem: str, n_samples: int, max_tokens: int = 512, temperature: float = 1.0) -> list[str]:\n",
    "    \"\"\"Generate multiple solutions for the given problem using the text-generation helper.\n",
    "    Tries generate_text(prompt) and falls back to generate_text(generator, prompt).\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for _ in range(n_samples):\n",
    "        try:\n",
    "            out = generate_text(problem, max_tokens=max_tokens, temperature=temperature)\n",
    "        except TypeError:\n",
    "            out = generate_text(generator, problem, max_tokens=max_tokens, temperature=temperature)\n",
    "        outputs.append(out)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def run_parallel_generation(test_data, n_samples: int, max_tokens: int = 512, temperature: float = 1.0):\n",
    "    sampled_responses = defaultdict(list)\n",
    "    for dataset_name, dataset in test_data.items():\n",
    "      for example in tqdm(dataset, desc=f\"{dataset_name}\"):\n",
    "        solutions = sample_solutions(example[\"problem\"], n_samples, max_tokens, temperature)\n",
    "        sampled_responses[dataset_name].append({\n",
    "            \"example\": example,\n",
    "            \"sampled_responses\": sample_solutions(example[\"problem\"], n_samples, max_tokens, temperature),\n",
    "        })\n",
    "    return sampled_responses\n",
    "\n",
    "\n",
    "SAMPLED_RESPONSES1 = run_parallel_generation(TEST_DATA, N_SAMPLES1, MAX_TOKENS1, TEMPERATURE1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fR_z_Kv71oVw"
   },
   "source": [
    "### Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEVveCcf1dWd"
   },
   "outputs": [],
   "source": [
    "def majority_voting(sampled_responses: list[str]) -> str:\n",
    "    \"\"\"Perform majority voting on the sampled responses.\n",
    "\n",
    "    Strategy:\n",
    "      1) Extract a final answer string for each response.\n",
    "         - Prefer \\boxed{...} if present.\n",
    "         - Else take the last number in the text (integer/decimal).\n",
    "         - Else fall back to the stripped text.\n",
    "      2) Return the most frequent extracted answer.\n",
    "         If there's a tie, return the first among the tied answers by order of appearance.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from collections import Counter\n",
    "\n",
    "    def extract_answer(text: str) -> str:\n",
    "        if text is None:\n",
    "            return \"\"\n",
    "        m = re.search(r\"\\\\boxed\\{([^}]*)\\}\", text)\n",
    "        if m:\n",
    "            return m.group(1).strip()\n",
    "        nums = re.findall(r\"-?\\d+(?:\\.\\d+)?\", text)\n",
    "        if nums:\n",
    "            return nums[-1]\n",
    "        return text.strip()\n",
    "\n",
    "    answers = [extract_answer(t) for t in sampled_responses]\n",
    "    if not answers:\n",
    "        return \"\"\n",
    "    counts = Counter(answers)\n",
    "    max_count = max(counts.values()) if counts else 0\n",
    "    for ans in answers:\n",
    "        if counts[ans] == max_count:\n",
    "            return ans\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def run_majority_voting(sampled_responses, n_samples: int = 0):\n",
    "  accuracy = defaultdict(int)\n",
    "\n",
    "  for dataset_name, examples in sampled_responses.items():\n",
    "    is_corrects = []\n",
    "    for example in examples:\n",
    "      sampled_responses = example[\"sampled_responses\"]\n",
    "      if n_samples > 0:\n",
    "        sampled_responses = sampled_responses[:n_samples]\n",
    "      majority_answer = majority_voting(sampled_responses)\n",
    "\n",
    "      is_correct = int(majority_answer == example[\"example\"][\"answer\"])\n",
    "      is_corrects.append(is_correct)\n",
    "\n",
    "    accuracy[dataset_name] = np.mean(is_correct)\n",
    "\n",
    "  return accuracy\n",
    "\n",
    "\n",
    "maj_at_n1 = run_majority_voting(SAMPLED_RESPONSES1)\n",
    "print(f\"majority voting@{N_SAMPLES1} = {maj_at_n1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKxe-RFo2DU_"
   },
   "source": [
    "### Varying N in Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsaLRYV53Ib0"
   },
   "outputs": [],
   "source": [
    "MAX_N_SAMPELS = 32 # @param {type:\"integer\"}\n",
    "assert MAX_N_SAMPELS > 0 and (MAX_N_SAMPELS & (MAX_N_SAMPELS - 1)) == 0, \"MAX_N_SAMPLES must be a power of 2 (for simplicty)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MD9NB6-0_5i"
   },
   "outputs": [],
   "source": [
    "SAMPLED_RESPONSES = run_parallel_generation(TEST_DATA, MAX_N_SAMPLES, MAX_TOKENS1, TEMPERATURE1)\n",
    "\n",
    "maj_at_n_data = defaultdict(list)\n",
    "\n",
    "for n in range(np.log2(MAX_N_SAMPLES) + 1):\n",
    "  n_samples = 2 ** n\n",
    "  maj_at_n = run_majority_voting(SAMPLED_RESPONSES, n_samples)\n",
    "  for dataset_name, accuracy in maj_at_n.items():\n",
    "    maj_at_n[dataset_name].append(\n",
    "        {\"n\": n_samples, \"accuracy\": accuracy, \"method\": \"majority voting\"}\n",
    "    )\n",
    "\n",
    "plot(maj_at_n, \"majority voting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pceDwQnL6ojg"
   },
   "source": [
    "# Part 2: Best-of-N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZJeVeU2-bfq"
   },
   "source": [
    "## Your task:\n",
    "  1. **Implement Best-of-N strategy given a reward model:** Reuse the parallel generations from Part 1. Keep the same parameters as Part 1.\n",
    "\n",
    "  2. **Vary `N` from 1 to 32 and make a comparison with Majority Voting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jdkw_JS7-gnr"
   },
   "source": [
    "## Your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbEQ2Yzn9sWo"
   },
   "outputs": [],
   "source": [
    "REWARD_MODEL_NAME = \"Skywork-Reward-V2-Llama-3.1-8B-40M\" # @param [\"Skywork-Reward-V2-Llama-3.1-8B-40M\", \"Skywork/Skywork-Reward-V2-Qwen3-8B\", \"Skywork-Reward-V2-Llama-3.2-3B\", \"Skywork-Reward-V2-Qwen3-4B\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJ20XYXfC4X-"
   },
   "source": [
    "### Loading Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5Lz2Ujl-jr0"
   },
   "outputs": [],
   "source": [
    "# off-load model from GPU\n",
    "del MODEL\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# load reward model\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(REWARD_MODEL_NAME)\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    REWARD_MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    num_labels=1,\n",
    ")\n",
    "\n",
    "\n",
    "def get_score(problem: str, response: str, model, tokenizer):\n",
    "  conversation = [\n",
    "      {\"role\": \"user\", \"content\": problem},\n",
    "      {\"role\": \"assistant\", \"content\": response},\n",
    "  ]\n",
    "\n",
    "  prompt = tokenizer.apply_chat_template(conversation, tokenize=False)\n",
    "  # following the recommendation in provided example: https://huggingface.co/Skywork/Skywork-Reward-V2-Qwen3-4B#%F0%9F%93%9D-simple-example-in-transformers\n",
    "  if tokenizer.bos_token is not None and prompt.startswith(tokenizer.bos_token):\n",
    "    prompt = prompt[len(tokenizer.bos_token):]\n",
    "\n",
    "  tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    score = model(tokenized_prompt).logits[0][0].item()\n",
    "\n",
    "  return score\n",
    "\n",
    "\n",
    "def get_batch_scores(examples: list, model, tokenizer):\n",
    "  \"\"\"\n",
    "  Compute reward scores in a batch for faster inference.\n",
    "  Each item can be either:\n",
    "    - dict with keys {\"problem\": str, \"response\": str}, or\n",
    "    - a plain response string (treated as assistant-only message).\n",
    "  Returns: list[float] scores in the same order.\n",
    "  \"\"\"\n",
    "  conversations = []\n",
    "  for ex in examples:\n",
    "    if isinstance(ex, dict) and \"problem\" in ex and \"response\" in ex:\n",
    "      conversations.append([\n",
    "          {\"role\": \"user\", \"content\": ex[\"problem\"]},\n",
    "          {\"role\": \"assistant\", \"content\": ex[\"response\"]},\n",
    "      ])\n",
    "    else:\n",
    "      resp = ex[\"response\"] if isinstance(ex, dict) and \"response\" in ex else str(ex)\n",
    "      conversations.append([{ \"role\": \"assistant\", \"content\": resp }])\n",
    "\n",
    "  prompts = [tokenizer.apply_chat_template(c, tokenize=False) for c in conversations]\n",
    "  if tokenizer.bos_token is not None:\n",
    "    prompts = [p[len(tokenizer.bos_token):] if isinstance(p, str) and p.startswith(tokenizer.bos_token) else p\n",
    "               for p in prompts]\n",
    "\n",
    "  batch = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "  with torch.no_grad():\n",
    "    logits = model(**batch).logits.squeeze(-1)\n",
    "  scores = logits.detach().float().cpu().numpy().tolist()\n",
    "  if isinstance(scores, float):\n",
    "    scores = [float(scores)]\n",
    "  else:\n",
    "    scores = [float(x) for x in scores]\n",
    "  return scores\n",
    "\n",
    "\n",
    "# SOLUTION: Implemented inference-time strategies\n",
    "import random\n",
    "from typing import List, Dict, Optional, Sequence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def simulate_candidates(n:int, p_correct:float, seed:Optional[int]=42) -> List[Dict]:\n",
    "    rng = random.Random(seed)\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        correct = rng.random() < p_correct\n",
    "        mu = 0.7 if correct else 0.3\n",
    "        score = max(0.0, min(1.0, rng.gauss(mu, 0.1)))\n",
    "        out.append({'text': f'candidate_{i}', 'is_correct': correct, 'score': score})\n",
    "    return out\n",
    "\n",
    "def best_of_n(cands: Sequence[Dict]) -> Dict:\n",
    "    return max(cands, key=lambda d: d['score'])\n",
    "\n",
    "def majority_vote(cands: Sequence[Dict]) -> bool:\n",
    "    votes = sum(1 for c in cands if c['is_correct'])\n",
    "    return votes > (len(cands)//2)\n",
    "\n",
    "def weighted_vote(cands: Sequence[Dict]) -> bool:\n",
    "    total = sum(c['score'] for c in cands)\n",
    "    correct = sum(c['score'] for c in cands if c['is_correct'])\n",
    "    if total == 0:\n",
    "        return False\n",
    "    return (correct / total) > 0.5\n",
    "\n",
    "def evaluate_strategies(num_trials:int=200, n:int=8, p_correct:float=0.4):\n",
    "    bo_hits = mv_hits = wv_hits = 0\n",
    "    for t in range(num_trials):\n",
    "        cands = simulate_candidates(n=n, p_correct=p_correct, seed=42+t)\n",
    "        bo_hits += int(best_of_n(cands)['is_correct'])\n",
    "        mv_hits += int(majority_vote(cands))\n",
    "        wv_hits += int(weighted_vote(cands))\n",
    "    return {\n",
    "        'best_of_n_acc': bo_hits/num_trials,\n",
    "        'majority_vote_acc': mv_hits/num_trials,\n",
    "        'weighted_vote_acc': wv_hits/num_trials,\n",
    "    }\n",
    "\n",
    "def sweep_n_plot(p_correct:float=0.4, max_n:int=15):\n",
    "    ns = list(range(1, max_n+1))\n",
    "    bo, mv, wv = [], [], []\n",
    "    for n in ns:\n",
    "        res = evaluate_strategies(num_trials=200, n=n, p_correct=p_correct)\n",
    "        bo.append(res['best_of_n_acc'])\n",
    "        mv.append(res['majority_vote_acc'])\n",
    "        wv.append(res['weighted_vote_acc'])\n",
    "    plt.figure()\n",
    "    plt.plot(ns, bo, label='Best-of-N')\n",
    "    plt.plot(ns, mv, label='Majority vote')\n",
    "    plt.plot(ns, wv, label='Weighted vote')\n",
    "    plt.xlabel('N (samples)')\n",
    "    plt.ylabel('Accuracy (simulated)')\n",
    "    plt.title(f'Strategy accuracy vs N (p_correct={p_correct})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11OvVc4LC8z0"
   },
   "source": [
    "### Best-of-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pz1CZLT_8_OL"
   },
   "outputs": [],
   "source": [
    "def best_of_n(sampled_responses: list[str], model, tokenizer) -> str:\n",
    "    \"\"\"Score each candidate with the reward model and return the best one.\n",
    "    Uses assistant-only chat templates (no user problem context available here).\n",
    "    \"\"\"\n",
    "    examples = [{\"response\": r} for r in sampled_responses]\n",
    "    if not examples:\n",
    "        return \"\"\n",
    "    scores = get_batch_scores(examples, model, tokenizer)\n",
    "    if not scores:\n",
    "        return sampled_responses[0]\n",
    "    best_idx = int(max(range(len(scores)), key=lambda i: scores[i]))\n",
    "    return sampled_responses[best_idx]\n",
    "\n",
    "\n",
    "def run_best_of_n(sampled_responses, model, tokenizer, n_samples: int = 0):\n",
    "  accuracy = defaultdict(int)\n",
    "\n",
    "  for dataset_name, examples in sampled_responses.items():\n",
    "    is_corrects = []\n",
    "    for example in examples:\n",
    "      sampled_responses = example[\"sampled_responses\"]\n",
    "      if n_samples > 0:\n",
    "        sampled_responses = sampled_responses[:n_samples]\n",
    "      best_answer = best_of_n(sampled_responses, model, tokenizer)\n",
    "\n",
    "      is_correct = int(best_answer == example[\"example\"][\"answer\"])\n",
    "      is_corrects.append(is_correct)\n",
    "\n",
    "    accuracy[dataset_name] = np.mean(is_correct)\n",
    "\n",
    "  return accuracy\n",
    "\n",
    "best_of_n1 = run_best_of_n(SAMPLED_RESPONSES1, reward_model, reward_tokenizer)\n",
    "print(f\"best-of-{N_SAMPLES1} = {best_of_n1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lljx4lyaE7Uk"
   },
   "source": [
    "### Varying N in Best-of-N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ruMwwfDDF0t"
   },
   "outputs": [],
   "source": [
    "best_of_n_data = defaultdict(list)\n",
    "\n",
    "for n in range(np.log2(MAX_N_SAMPLES) + 1):\n",
    "  n_samples = 2 ** n\n",
    "  best_of_n = run_best_of_n(SAMPLED_RESPONSES, reward_model, reward_tokenizer, n_samples)\n",
    "  for dataset_name, accuracy in maj_at_n.items():\n",
    "    best_of_n[dataset_name].append(\n",
    "        {\"n\": n_samples, \"accuracy\": accuracy, \"method\": \"best-of-N\"}\n",
    "    )\n",
    "\n",
    "plot(best_of_n_data, \"Best-of-N\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8AWWRNNgBQT"
   },
   "source": [
    "# Part 3: Self-correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e7oPfLiFJP7"
   },
   "source": [
    "\n",
    "## Your tasks:\n",
    "  1. **Implement self-correction:** Given sampled responses from Part 1 (use the first response from parallel generations), prompt the same model for a self-verification and collect the new answer.\n",
    "\n",
    "  2. **Measure the accuracy after self-correction and compare it with before**\n",
    "\n",
    "  3. **Bonus: Plot accuracy as a function of the number of output tokens**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUFfZyC5FK2z"
   },
   "source": [
    "## Your code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWEVtOckFOI_"
   },
   "source": [
    "### Loading model (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWkHd2kPFXMZ"
   },
   "outputs": [],
   "source": [
    "# off-load reward model from GPU\n",
    "del reward_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print(f\"Loading {SELECTED_MODEL}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(SELECTED_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(SELECTED_MODEL)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "generator = pipeline(\n",
    "  \"text-generation\",\n",
    "  model=model,\n",
    "  tokenizer=tokenizer,\n",
    "  model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "  device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"Model loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGKstv20FSSN"
   },
   "source": [
    "### Self-correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AiJNyJEGgGLO"
   },
   "outputs": [],
   "source": [
    "def self_correct(problem: str, response: str, model):\n",
    "    \"\"\"Use the model to verify/correct a proposed answer and return a new final answer inside \\\\boxed{...}.\"\"\"\n",
    "    import re\n",
    "    verify_prompt = (\n",
    "        \"You are checking a solution to a math problem.\\n\"\n",
    "        f\"Problem: {problem}\\n\"\n",
    "        f\"Proposed answer: {response}\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        \"1) Re-compute the correct answer carefully.\\n\"\n",
    "        \"2) If the proposed answer is correct, keep it. If it's wrong, fix it.\\n\"\n",
    "        \"3) Reply with ONLY the final answer inside \\\\boxed{...} and nothing else.\"\n",
    "    )\n",
    "    try:\n",
    "        out = generate_text(verify_prompt, max_tokens=128, temperature=0.0)\n",
    "    except TypeError:\n",
    "        out = generate_text(generator, verify_prompt, max_tokens=128, temperature=0.0)\n",
    "    m = re.search(r\"\\\\\\boxed\\{([^}]*)\\}\", out or \"\")\n",
    "    return m.group(1).strip() if m else (out or \"\").strip()\n",
    "\n",
    "\n",
    "\n",
    "def run_self_correct(sampled_responses, model):\n",
    "  accuracy = defaultdict(int)\n",
    "\n",
    "  for dataset_name, examples in sampled_responses.items():\n",
    "    is_corrects = []\n",
    "    for example in examples:\n",
    "      response = example[\"sampled_responses\"][0]\n",
    "      modified_answer = self_correct(example[\"example\"][\"problem\"], response, model)\n",
    "\n",
    "      is_correct = int(modified_answer == example[\"example\"][\"answer\"])\n",
    "      is_corrects.append(is_correct)\n",
    "\n",
    "    accuracy[dataset_name] = np.mean(is_correct)\n",
    "\n",
    "  return accuracy\n",
    "\n",
    "\n",
    "self_correct = run_self_correct(SAMPLED_RESPONSES1, model)\n",
    "print(f\"self-correct = {self_correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP_2nO6zgMqQ"
   },
   "source": [
    "## Reflection Questions:\n",
    "\n",
    "1. Which strategy worked best for multiplication problems? Why?\n",
    "2. How did compute budget affect your results?\n",
    "3. What are the trade-offs between the two main approaches?\n",
    "4. How would you extend these methods to harder problems?\n",
    "5. What external information would be most helpful?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "tKU-AQRJsxDy"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
